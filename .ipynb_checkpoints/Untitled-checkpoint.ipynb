{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import scipy.io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def load_data(image_paths, labels, crop_size):\n",
    "    \"\"\"\n",
    "    Given list of paths, load images as one numpy array of shape\n",
    "        (num_images, crop_size, crop_size, channel)\n",
    "    :return X: image array\n",
    "    \"return y: one hot encoded labels\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(image_paths), crop_size, crop_size, 3))\n",
    "    for i,path in enumerate(image_paths):\n",
    "        X[i, :] = img_to_array(load_img(path, target_size=(crop_size, crop_size)))\n",
    "    y = np_utils.to_categorical(labels, 197)\n",
    "    return X, y\n",
    "\n",
    "crop_size = 244\n",
    "all_per_class = 16\n",
    "train_paths = []\n",
    "val_paths = []\n",
    "test_paths = []\n",
    "train_labels = []\n",
    "val_labels = []\n",
    "test_labels = []\n",
    "\n",
    "train_anno = scipy.io.loadmat('./devkit/cars_train_annos.mat')['annotations'][0]\n",
    "for i in train_anno:\n",
    "    train_labels += [i[-2][0][0]]\n",
    "    train_paths += ['./cars_train/' + str(i[-1][0])]\n",
    "\n",
    "test_anno = scipy.io.loadmat('./devkit/cars_test_annos.mat')['annotations'][0]\n",
    "for i in train_anno:\n",
    "    test_labels += [i[-2][0][0]]\n",
    "    test_paths += ['./cars_test/' + str(i[-1][0])]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, y_train = load_data(train_paths[:3000], train_labels[:3000], crop_size)\n",
    "#X_val, y_val = load_data(val_paths, val_labels, crop_size)\n",
    "X_test, y_test = load_data(test_paths[:1000], test_labels[:1000], crop_size)\n",
    "#X_train = preprocess_input(X_train)\n",
    "#X_val = preprocess_input(X_val)\n",
    "X_test = preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_arrays(batch_size):\n",
    "    sample_number = 8144\n",
    "    crop_size = 244\n",
    "    while True:\n",
    "        for i in range(0,sample_number,batch_size):\n",
    "            if i + batch_size > sample_number:\n",
    "                X_train, y_train = load_data(train_paths[i:], train_labels[i:], crop_size)\n",
    "                X_train = preprocess_input(X_train)\n",
    "            else:\n",
    "                X_train, y_train = load_data(train_paths[i:i+batch_size], train_labels[i:i+batch_size], crop_size)\n",
    "                X_train = preprocess_input(X_train)\n",
    "            yield X_train, y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model( output_dim ):\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    # and a logistic layer|\n",
    "    predictions = Dense(output_dim, activation='softmax')(x)\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "def get_model2(output_dim):\n",
    "    vgg_model = VGG16( weights='imagenet', include_top=True )\n",
    "    vgg_out = vgg_model.layers[-2].output #Last FC layer's output \n",
    "    softmax_layer = Dense(output_dim=output_dim, activation='softmax')(vgg_out)\n",
    "    #Create new transfer learning model\n",
    "    tl_model = Model( input=vgg_model.input, output=softmax_layer )\n",
    "\n",
    "    #Freeze all layers of VGG16 and Compile the model\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "    #Confirm the model is appropriate\n",
    "    #tl_model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #tl_model.summary()\n",
    "    return tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def class_map():\n",
    "    a = scipy.io.loadmat('./devkit/cars_meta.mat')\n",
    "    res = {a['class_names'][0][i][0]: i for i in range(0, len(a['class_names'][0]))}\n",
    "    res2 = {i : a['class_names'][0][i][0] for i in range(0, len(a['class_names'][0]))}\n",
    "    return res,res2\n",
    "c2i, i2c = class_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_model2(197)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='./model/test.{epoch:02d}-{val_loss:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected input_5 to have shape (None, 224, 224, 3) but got array with shape (1000, 299, 299, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-859460a0ee4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                    callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\SuperZ\\Anaconda2\\envs\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1501\u001b[0m                                  str(validation_data))\n\u001b[1;32m   1502\u001b[0m             val_x, val_y, val_sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1503\u001b[0;31m                 val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m   1504\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SuperZ\\Anaconda2\\envs\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m   1030\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SuperZ\\Anaconda2\\envs\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: expected input_5 to have shape (None, 224, 224, 3) but got array with shape (1000, 299, 299, 3)"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "#              optimizer=optimizers.SGD(lr=0.0001 , momentum=0.9, nesterov=True))\n",
    "model.fit_generator(generate_arrays(batch_size=64),\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    samples_per_epoch=len(test_labels)/4, nb_epoch=10,\n",
    "                   callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test[0].reshape(1,299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0993364e-26"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8144"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
